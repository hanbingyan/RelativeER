{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instance = 10\n",
    "n_agents = 2\n",
    "actions_space = np.arange(1.2, 2.3, 0.05)\n",
    "quality = np.ones(n_agents) * 2\n",
    "margin_cost = np.ones(n_agents)\n",
    "margin_cost[1] = 0.5\n",
    "horizon = 1 / 4\n",
    "a0 = 0\n",
    "n_actions = actions_space.size\n",
    "state_ravel = (n_actions,) * n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2 , 1.25, 1.3 , 1.35, 1.4 , 1.45, 1.5 , 1.55, 1.6 , 1.65, 1.7 ,\n",
       "       1.75, 1.8 , 1.85, 1.9 , 1.95, 2.  , 2.05, 2.1 , 2.15, 2.2 , 2.25])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_classic_reward(price):\n",
    "    # Compute profits for all agents\n",
    "#     price = actions_space[action]\n",
    "    demand = np.exp((quality - price) / horizon)\n",
    "    demand = demand / (np.sum(demand) + np.exp(a0 / horizon))\n",
    "    reward = np.multiply(price - margin_cost, demand)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('asyact_price.pickle', 'rb') as fp:\n",
    "    a_state = pickle.load(fp)\n",
    "\n",
    "with open('asyact_Q.pickle', 'rb') as fp:\n",
    "    a_Q = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('asyreward_Q.pickle', 'rb') as fp:\n",
    "    r_Q = pickle.load(fp)\n",
    "\n",
    "with open('asyreward_price.pickle', 'rb') as fp:\n",
    "    r_state = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Q[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0 vs 0 0.3564631708083744 ratio 0.3677252347145463\n",
      "Instance 1 vs 1 0.42472570912413754 ratio 0.7343078212319304\n",
      "Instance 2 vs 2 0.4059383283814131 ratio 0.6334160759719324\n",
      "Instance 3 vs 3 0.3752130741660772 ratio 0.46841571941383076\n",
      "Instance 4 vs 4 0.356753990886896 ratio 0.36928699287980077\n",
      "Instance 5 vs 5 0.41313305028778613 ratio 0.6720530770384819\n",
      "Instance 6 vs 6 0.3753375530735734 ratio 0.46908419440762233\n",
      "Instance 7 vs 7 0.39120624398290804 ratio 0.5543020298663646\n",
      "Instance 8 vs 8 0.3998168714132922 ratio 0.6005427081776025\n",
      "Instance 9 vs 9 0.36627096786040814 ratio 0.4203949376010834\n"
     ]
    }
   ],
   "source": [
    "# Reward matrix calculator\n",
    "count = 0\n",
    "N = 100000\n",
    "monopoly = replay_classic_reward(np.array([2.198, 1.698])).mean()\n",
    "nash = replay_classic_reward(np.array([1.372, 1.204])).mean()\n",
    "\n",
    "# ss = np.random.SeedSequence(12345)\n",
    "# # Last a few rngs for sampling memories\n",
    "# child_seeds = ss.spawn(n_agents)\n",
    "# rng = [np.random.default_rng(s) for s in child_seeds]\n",
    "\n",
    "ratio = np.zeros(n_instance)\n",
    "\n",
    "# converged price\n",
    "converged = np.zeros(n_agents)\n",
    "\n",
    "for agent0 in range(n_instance):\n",
    "    for agent1 in [agent0]:\n",
    "        sim_Q = np.zeros((n_agents, n_actions**n_agents, n_actions))\n",
    "        sim_Q[0, :, :] = r_Q[agent0][0, :, :]\n",
    "        sim_Q[1, :, :] = r_Q[agent1][1, :, :]\n",
    "        state = r_state[agent0][-1]\n",
    "#         init = np.zeros(n_agents, dtype=int)\n",
    "#         for i in range(n_agents):\n",
    "#             init[i] = rng[i].integers(0, n_actions, size=1)\n",
    "#         state = np.ravel_multi_index(init, state_ravel)\n",
    "#         state = np.random.randint(0, n_actions**n_agents, size=1)\n",
    "        action = np.zeros(n_agents, dtype=int)\n",
    "        reward = np.zeros(n_agents)\n",
    "        for k in range(N):\n",
    "            # For each agent, select and perform an action\n",
    "            for i in range(n_agents):\n",
    "                action[i] = sim_Q[i, state].argmax()\n",
    "            if k > N - 10001:\n",
    "                converged += actions_space[action]\n",
    "                if action[0] > action[1]:\n",
    "                    count += 1\n",
    "#                 print('Instance', agent0, 'Instance', agent1, action)\n",
    "                reward += replay_classic_reward(actions_space[action])\n",
    "#             # Move to the next state\n",
    "            state = np.ravel_multi_index(action, state_ravel)\n",
    "            avg = np.sum(reward)/10001/n_agents\n",
    "            ratio[agent0] = (avg - nash)/(monopoly - nash)\n",
    "        print('Instance', agent0, 'vs', agent1, avg, 'ratio', ratio[agent0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.785285, 1.870372])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converged/10000/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action based ratio\n",
    "a_ratio = np.array([0.60938436, 0.79874612, 0.61134698, 0.63048103, 0.61707901,\n",
    "                    0.51815132, 0.73782273, 0.7805825 , 0.64445768, 0.63972649])\n",
    "# count/10000/10 = 0.5334399999999999\n",
    "# Average price array([1.8161735, 1.7340945])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward based profit ratio\n",
    "r_ratio = np.array([0.36772523, 0.73427518, 0.63341608, 0.4683863 , 0.36928699,\n",
    "                    0.67205308, 0.46897823, 0.55435253, 0.60065238, 0.42052928])\n",
    "# count/10000/10 = 0.3325\n",
    "# average price array([1.785285, 1.870372])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.658777822, 0.08261254899344067)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ratio.mean(), a_ratio.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.528965528, 0.12240993874629502)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_ratio.mean(), r_ratio.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_hist = np.zeros((n_instance, 20, n_agents), dtype=int)\n",
    "# price_hist = np.zeros((n_instance, 20, n_agents))\n",
    "# reward_hist = np.zeros((n_instance, 20, n_agents))\n",
    "\n",
    "# count = 0\n",
    "# for ins in range(n_instance):\n",
    "#     for k in range(20):\n",
    "#         act_hist[ins, k, :] = np.unravel_index(r_state[ins][k], state_ravel)\n",
    "#         price_hist[ins, k, :] = actions_space[act_hist[ins, k, :]]\n",
    "#         reward_hist[ins, k, :] = replay_classic_reward(price_hist[ins, k, :])\n",
    "#         if price_hist[ins, k, 0] > price_hist[ins, k, 1]:\n",
    "#             count += 1\n",
    "# (reward_hist.mean(axis=(1,2)) - nash)/(monopoly - nash)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
